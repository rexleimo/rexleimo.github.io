<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Rust Async Microservices, Tamed: Rate Limiting, Backpressure, Batching, and Middleware | 梦兽编程</title><meta name=keywords content="Rust,async,Tokio,Axum,Tower,microservices,high-concurrency,rate limiting,backpressure,batching,middleware,performance optimization"><meta name=description content="When traffic surges like a flood, equip your Rust microservices with four gates—rate limiting, backpressure, batching, and middleware—using Tokio + Tower. Keep P99 steady and make speed happen in order."><meta name=author content="Rexai Programming"><link rel=canonical href=https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/><link crossorigin=anonymous href=/assets/css/stylesheet.36a35dd316f612e0d26c90cd90313d15705d24943d7cf2559674e92b21704616.css integrity="sha256-NqNd0xb2EuDSbJDNkDE9FXBdJJQ9fPJVlnTpKyFwRhY=" rel="preload stylesheet" as=style><link rel=icon href=https://rexai.top/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rexai.top/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rexai.top/favicon-32x32.png><link rel=apple-touch-icon href=https://rexai.top/apple-touch-icon.png><link rel=mask-icon href=https://rexai.top/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-cn href=https://rexai.top/tutorials/rust/rust-async-microservices-traffic-gates-20250820/><link rel=alternate hreflang=en href=https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=/css/extended/modern-typography.min.css integrity><style>.promotion-section{margin-top:3rem;padding:2rem;text-align:center;background:linear-gradient(135deg,rgba(59,130,246,5%) 0%,rgba(139,92,246,5%) 100%);border-radius:16px;border:1px solid rgba(59,130,246,.1);position:relative;overflow:hidden}.promotion-section::before{content:"";position:absolute;top:0;left:0;right:0;height:4px;background:linear-gradient(90deg,#3b82f6,#8b5cf6,#06b6d4);border-radius:16px 16px 0 0}.promotion-section h3{margin-bottom:1.5rem;color:var(--primary);font-size:1.5rem;font-weight:600;background:linear-gradient(135deg,#3b82f6,#8b5cf6);-webkit-background-clip:text;-webkit-text-fill-color:transparent;background-clip:text}.qr-codes{display:flex;gap:2.5rem;justify-content:center;flex-wrap:wrap;margin-top:2rem}.qr-item{text-align:center;position:relative;transition:transform .3s ease}.qr-item:hover{transform:translateY(-5px)}.qr-item img{width:160px;height:160px;border:2px solid #e5e7eb;border-radius:16px;padding:12px;background:#fff;box-shadow:0 4px 20px rgba(0,0,0,8%),0 1px 3px rgba(0,0,0,.1);transition:all .3s ease;position:relative;overflow:hidden}.qr-item img::before{content:"";position:absolute;top:0;left:0;right:0;bottom:0;background:linear-gradient(135deg,rgba(59,130,246,.1),rgba(139,92,246,.1));opacity:0;transition:opacity .3s ease;z-index:1}.qr-item:hover img{border-color:#3b82f6;box-shadow:0 8px 30px rgba(59,130,246,.2),0 2px 8px rgba(0,0,0,.1);transform:scale(1.02)}.qr-item:hover img::before{opacity:1}.qr-item p{margin-top:1rem;font-size:1rem;color:var(--secondary);font-weight:500;position:relative}.modern-button{display:inline-flex;align-items:center;padding:.75rem 1.5rem;background:linear-gradient(135deg,#3b82f6,#8b5cf6);color:#fff;text-decoration:none;border-radius:12px;font-weight:500;transition:all .3s ease;box-shadow:0 4px 12px rgba(59,130,246,.3);position:relative;overflow:hidden}.modern-button::before{content:"";position:absolute;top:0;left:-100%;width:100%;height:100%;background:linear-gradient(90deg,transparent,rgba(255,255,255,.2),transparent);transition:left .6s ease}.modern-button:hover::before{left:100%}.modern-button:hover{transform:translateY(-2px);box-shadow:0 6px 20px rgba(59,130,246,.4)}@keyframes fadeInUp{from{opacity:0;transform:translateY(30px)}to{opacity:1;transform:translateY(0)}}.post-content>*{animation:fadeInUp .6s ease-out;animation-fill-mode:both}.post-content>*:nth-child(1){animation-delay:.1s}.post-content>*:nth-child(2){animation-delay:.2s}.post-content>*:nth-child(3){animation-delay:.3s}.post-content>*:nth-child(4){animation-delay:.4s}@media(max-width:768px){.promotion-section{margin:2rem -1rem;border-radius:0;padding:1.5rem 1rem}.qr-codes{flex-direction:column;align-items:center;gap:1.5rem}.qr-item img{width:140px;height:140px;padding:10px}.promotion-section h3{font-size:1.25rem}}.reading-progress{position:fixed;top:0;left:0;width:0%;height:3px;background:linear-gradient(90deg,#3b82f6,#8b5cf6,#06b6d4);z-index:999;transition:width .1s ease}.post-actions{margin-top:2.5rem;padding-top:1.25rem;border-top:1px dashed var(--border)}.post-actions__group{display:flex;flex-wrap:wrap;gap:.75rem 1rem;align-items:center}.post-actions__item{display:inline-flex;align-items:center}*{transition:background-color .3s ease,color .3s ease,border-color .3s ease}::-webkit-scrollbar{width:8px}::-webkit-scrollbar-track{background:var(--theme)}::-webkit-scrollbar-thumb{background:linear-gradient(135deg,#3b82f6,#8b5cf6);border-radius:4px}::-webkit-scrollbar-thumb:hover{background:linear-gradient(135deg,#2563eb,#7c3aed)}</style><script>document.addEventListener("DOMContentLoaded",function(){const e=document.createElement("div");e.className="reading-progress",document.body.appendChild(e);function t(){const t=document.documentElement.scrollTop||document.body.scrollTop,n=document.documentElement.scrollHeight-document.documentElement.clientHeight,s=t/n*100;e.style.width=s+"%"}window.addEventListener("scroll",t),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();const t=document.querySelector(this.getAttribute("href"));t&&t.scrollIntoView({behavior:"smooth",block:"start"})})})})</script><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-K6VQRF8T")</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K6VQRF8T" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-62VF26DEY6"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-62VF26DEY6")</script><meta property="og:url" content="https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/"><meta property="og:site_name" content="梦兽编程"><meta property="og:title" content="Rust Async Microservices, Tamed: Rate Limiting, Backpressure, Batching, and Middleware"><meta property="og:description" content="When traffic surges like a flood, equip your Rust microservices with four gates—rate limiting, backpressure, batching, and middleware—using Tokio + Tower. Keep P99 steady and make speed happen in order."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2025-08-20T10:00:00+08:00"><meta property="article:modified_time" content="2025-08-21T10:00:00+08:00"><meta property="article:tag" content="Tokio"><meta property="article:tag" content="Axum"><meta property="article:tag" content="Tower"><meta property="article:tag" content="Rate-Limiting"><meta property="article:tag" content="Backpressure"><meta property="article:tag" content="Batching"><meta property="og:image" content="https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover.png"><meta name=twitter:title content="Rust Async Microservices, Tamed: Rate Limiting, Backpressure, Batching, and Middleware"><meta name=twitter:description content="When traffic surges like a flood, equip your Rust microservices with four gates—rate limiting, backpressure, batching, and middleware—using Tokio + Tower. Keep P99 steady and make speed happen in order."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Tutorials","item":"https://rexai.top/en/tutorials/"},{"@type":"ListItem","position":2,"name":"Rust Async Microservices, Tamed: Rate Limiting, Backpressure, Batching, and Middleware","item":"https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Rust Async Microservices, Tamed: Rate Limiting, Backpressure, Batching, and Middleware","name":"Rust Async Microservices, Tamed: Rate Limiting, Backpressure, Batching, and Middleware","description":"When traffic surges like a flood, equip your Rust microservices with four gates—rate limiting, backpressure, batching, and middleware—using Tokio + Tower. Keep P99 steady and make speed happen in order.","keywords":["Rust","async","Tokio","Axum","Tower","microservices","high-concurrency","rate limiting","backpressure","batching","middleware","performance optimization"],"articleBody":"Bold claim up front: stability is not “slow.” Stability is making “fast” happen in an orderly way. With the gates placed at the right spots, even a flood can be channeled into rivers.\nThink about a subway during Monday rush hour. Entry turnstiles regulate inflow, platforms enforce headcount, trains add extra cars, and the PA system orchestrates everything. That is exactly what microservices must do under high concurrency. Your Rust service needs the same four gates: rate limiting, backpressure, batching, and a smart brain of middleware.\nWhy congestion happens: async is not infinite concurrency Async doesn’t mean your server can accept infinite work like it’s cheating. Reality looks more like a food delivery platform: a burst of orders gets constrained by riders, kitchen capacity, and merchant speed. Throughput is always limited by the slowest stage. Queues are not sinful by themselves—uncontrolled queues cause incidents.\nFirst principle: don’t let requests pile up where you don’t see them. Reject when you must, queue when you should, batch when it helps, coordinate when it matters.\nRate limiting: the turnstile at the entrance Entrance rate limiting is like the turnstile. We’re not unfriendly—we just don’t want everyone to rush the platform and trample each other. Limiting protects yourself and downstream dependencies, especially third‑party APIs, databases, and expensive resources.\nIn Rust with the Tower ecosystem, you can add this gate cleanly. Here’s a minimal stack that bundles rate limiting, concurrency caps, timeouts, and tracing, reusable across Axum routes:\nuse axum::{routing::post, Router}; use std::time::Duration; use tower::{ServiceBuilder, timeout::TimeoutLayer}; use tower::limit::{ConcurrencyLimitLayer, RateLimitLayer}; use tower_http::trace::TraceLayer; async fn create_order() -\u003e \u0026'static str { \"ok\" } #[tokio::main] async fn main() { let middleware_stack = ServiceBuilder::new() .layer(TraceLayer::new_for_http()) .layer(TimeoutLayer::new(Duration::from_secs(2))) .layer(ConcurrencyLimitLayer::new(256)) .layer(RateLimitLayer::new(100, Duration::from_secs(1))) .into_inner(); let app = Router::new() .route(\"/orders\", post(create_order)) .layer(middleware_stack); axum::Server::bind(\u0026\"0.0.0.0:3000\".parse().unwrap()) .serve(app.into_make_service()) .await .unwrap(); } Two layers are doing heavy lifting here: RPS smoothing (max 100 per second) and in‑flight capping (max 256 concurrent). This both smooths bursts and prevents downstream collapse. You can go further with tenant/API‑key buckets to make fairness measurable.\nBackpressure: when the kitchen rail is full, stop taking orders When the kitchen ticket rail is full, a good manager doesn’t keep jamming tickets in. They ask customers to retry later or go to another window. That’s backpressure: “I’m busy now—give me a breath.”\nIn Rust, the first hammer is boundedness. Whether it’s a queue or a buffer, set an upper bound. When full, either wait or fail fast:\nuse tokio::sync::mpsc; use tokio::time::{sleep, Duration}; #[derive(Debug)] struct Job(u64); #[tokio::main] async fn main() { // Bounded queue with capacity 1024 let (tx, mut rx) = mpsc::channel::\u003cJob\u003e(1024); // Producer: try_send fails if the queue is full; we fail fast here let producer = tokio::spawn(async move { for i in 0..10_000u64 { if tx.try_send(Job(i)).is_err() { // Backpressure signal: drop or record and let upstream retry } } }); // Consumer: simulate processing latency let consumer = tokio::spawn(async move { while let Some(job) = rx.recv().await { let _ = job; sleep(Duration::from_millis(2)).await; } }); let _ = tokio::join!(producer, consumer); } At the HTTP entrance, add “busy? instantly 503” to tell upstream to back off instead of building a latency snowball:\nuse tower::ServiceBuilder; use tower::load_shed::LoadShedLayer; let app = Router::new() .route(\"/orders\", post(create_order)) .layer(ServiceBuilder::new() .layer(LoadShedLayer::new()) ); The biggest pitfall is unboundedness. Unbounded queues drag tail latency into the unacceptable; blind retries under load create retry storms. Make failures arrive sooner and make retries smarter (exponential backoff with caps).\nBatching: fill a bucket, then flush—more throughput, less effort A laundromat doesn’t run a machine for one shirt; couriers deliver multiple packages to the same block in one run. Many write paths benefit from batching: bulk DB writes, batch MQ publishes, or downstream APIs that support batch endpoints.\nA common approach is dual thresholds: flush when you hit N items or T milliseconds—whichever comes first. Skeleton:\nuse tokio::{sync::mpsc, time::{self, Duration, Instant}}; #[derive(Clone, Debug)] struct Event(String); #[tokio::main] async fn main() { let (tx, mut rx) = mpsc::channel::\u003cEvent\u003e(2048); let batcher = tokio::spawn(async move { let max_batch = 100usize; let max_wait = Duration::from_millis(50); let mut buf = Vec::with_capacity(max_batch); let mut deadline = Instant::now() + max_wait; let mut ticker = time::interval(max_wait); ticker.set_missed_tick_behavior(time::MissedTickBehavior::Delay); loop { tokio::select! { maybe = rx.recv() =\u003e { match maybe { Some(ev) =\u003e { buf.push(ev); if buf.len() \u003e= max_batch { flush(\u0026mut buf).await; deadline = Instant::now() + max_wait; } } None =\u003e { if !buf.is_empty() { flush(\u0026mut buf).await; } break; } } } _ = ticker.tick() =\u003e { if !buf.is_empty() { flush(\u0026mut buf).await; } deadline = Instant::now() + max_wait; } } } }); let _ = batcher.await; } async fn flush(buf: \u0026mut Vec\u003cEvent\u003e) { // Batch write to downstream (DB/queue/batch API) // Record batch size and latency for tuning buf.clear(); } Batching boosts throughput but you must watch tail latency. Start with small batches and short waits, observe P95/P99, then dial up slowly. Emit metrics for batch size, wait time, and failure rate.\nMiddleware: rules in one brain, elegance everywhere Airport security has multiple stations, coordinated but non‑blocking. Middleware is where you centralize rules: timeouts, retries, rate limiting, compression, tracing, auth. Configure once, benefit many times.\nIn Tower/Axum, a common stack looks like:\nuse std::time::Duration; use tower::{ServiceBuilder, timeout::TimeoutLayer}; use tower::limit::{ConcurrencyLimitLayer, RateLimitLayer}; use tower_http::{trace::TraceLayer, compression::CompressionLayer, classify::ServerErrorsFailureClass}; use tower_http::cors::CorsLayer; let middleware_stack = ServiceBuilder::new() .layer(TraceLayer::new_for_http()) .layer(CompressionLayer::new()) .layer(CorsLayer::permissive()) .layer(TimeoutLayer::new(Duration::from_secs(1))) .layer(ConcurrencyLimitLayer::new(256)) .layer(RateLimitLayer::new(100, Duration::from_secs(1))) .into_inner(); Pro tip: extract parameters into config and make them metric‑driven. When the system is hot, automatically shrink the concurrency cap; when downstream is healthy, ease it open. Tower makes this natural.\nChoosing a strategy: a one‑liner matrix If upstream is too aggressive: apply rate limiting at the entrance, then backpressure to prevent buildup. If downstream is expensive: prioritize batching with timeouts and smart retries. If the entire chain is jittery: steady the entrance with rate limiting, then make concurrency/timeout/retry parameters dynamic and metric‑driven. One practical flow: keeping checkout stable on a big sale At the entrance, rate limit by tenant/user and reserve burst budgets for power users. Writes use batching: “100 items or 50 ms, whichever first.” Place a concurrency cap before inventory service; when busy, instantly 503 (load shed). Upstream retries with exponential backoff. Use tracing for end‑to‑end spans and metrics for exposure. Dashboards focus on in/out QPS, queue depth, batch size, timeout rate, and P95/P99. Observe and optimize: the stability flywheel Without data, tuning is like walking in the dark. At minimum, capture: in/out traffic, current concurrency, queue depth, batch size, timeouts/retries, and P95/P99 latency. Incidents should make it obvious which segment is red. With these, you can automate: when SLO alerts fire, temporarily reduce entrance RPS by 20% or shorten batch wait by 30% to ride out peaks, then restore.\nFinal hammer: stability isn’t slow—it’s ordered speed Install the four gates on your critical endpoints. Put the gate at the entrance so floods don’t slam the kitchen. Then you’ll find that even under massive load, the system is “busy but orderly.”\n","wordCount":"1130","inLanguage":"en","image":"https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover.png","datePublished":"2025-08-20T10:00:00+08:00","dateModified":"2025-08-21T10:00:00+08:00","author":{"@type":"Person","name":"Rexai Programming"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://rexai.top/en/tutorials/rust/rust-async-microservices-traffic-gates-20250820/"},"publisher":{"@type":"Organization","name":"梦兽编程","logo":{"@type":"ImageObject","url":"https://rexai.top/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://rexai.top/en/ accesskey=h title="梦兽编程 (Alt + H)">梦兽编程</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://rexai.top/ title=中文 aria-label=中文>Zh-Cn</a></li></ul></div></div><button class=menu-toggle id=menu-toggle aria-label=打开菜单 aria-controls=menu aria-expanded=false>
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul id=menu role=menubar><li role=none><a href=https://rexai.top/en/ title=Home role=menuitem><span>Home</span></a></li><li class=has-submenu role=none><a href=https://rexai.top/en/tutorials/ title="📚 Tutorials" role=menuitem><span>📚 Tutorials</span>
<span class=submenu-toggle role=button tabindex=0 aria-expanded=false aria-haspopup=true>▼</span></a><ul class=submenu role=menu><li role=none><a href=https://rexai.top/en/tutorials/rust/ title=Rust role=menuitem>Rust</a></li><li role=none><a href=https://rexai.top/en/tutorials/ai/ title="AI Programming" role=menuitem>AI Programming</a></li></ul></li><li role=none><a href=https://rexai.top/en/tools/ title=Tools role=menuitem><span>Tools</span></a></li><li role=none><a href=https://rexai.top/en/news/ title="🔥 News" role=menuitem><span>🔥 News</span></a></li><li role=none><a href=https://rexai.top/en/tags/ title="🏷️ Tags" role=menuitem><span>🏷️ Tags</span></a></li><li role=none><a href=https://rexai.top/en/categories/ title="📂 Categories" role=menuitem><span>📂 Categories</span></a></li></ul><div id=menu-overlay class=menu-overlay hidden></div></nav></header><style>.has-submenu{position:relative}#menu{overflow:visible !important}#menu li{overflow:visible}.submenu{position:absolute;top:100%;left:0;background:var(--theme);border:1px solid var(--border);border-radius:8px;box-shadow:0 4px 6px -1px rgba(0,0,0,.1);min-width:180px;z-index:1000;list-style:none;display:none;padding:.5rem 0}.submenu li{display:block;margin:0;line-height:1.5}#menu .submenu li+li{margin-inline-start:0}.submenu a{display:block;padding:.75rem 1rem;color:var(--secondary);text-decoration:none;transition:all .2s ease;border-radius:0}.submenu a:hover{background:var(--entry);color:var(--primary)}.has-submenu:hover .submenu{display:block !important}.submenu-toggle{margin-left:.25rem;font-size:.75rem;transition:transform .2s ease;display:inline-block}.has-submenu:hover .submenu-toggle{transform:rotate(180deg)}@media(max-width:768px){.menu-toggle{display:inline-flex;align-items:center;justify-content:center;width:44px;height:44px;margin-inline-end:var(--gap);border-radius:8px;border:1px solid var(--border);background:var(--theme);color:var(--primary)}#menu{position:fixed;top:0;right:0;height:100vh;width:82vw;max-width:340px;background:var(--theme);box-shadow:-2px 0 20px rgba(0,0,0,.18);transform:translateX(100%);transition:transform .25s ease;padding:calc(var(--header-height) + 10px)16px 20px;flex-direction:column;gap:4px;overflow-y:auto !important;z-index:1001}body.menu-open #menu{transform:translateX(0)}.menu-overlay{position:fixed;inset:0;background:rgba(0,0,0,.4);opacity:0;pointer-events:none;transition:opacity .2s ease;z-index:1000}body.menu-open .menu-overlay{opacity:1;pointer-events:auto}#menu li+li{margin-inline-start:0}#menu a{padding:12px 8px}.submenu{position:static;box-shadow:none;border:none;background:var(--entry);margin-top:0;margin-left:1rem;display:block;max-height:0;overflow:hidden;padding:0;transition:max-height .25s ease,padding .2s ease}.has-submenu.active .submenu{max-height:1000px;padding:4px 0 6px}.submenu-toggle{cursor:pointer}.has-submenu.active>a .submenu-toggle{transform:rotate(180deg)}}@media(min-width:769px){.menu-toggle{display:none}}.submenu a:focus-visible,#menu a:focus-visible,.menu-toggle:focus-visible,.submenu-toggle:focus-visible{outline:2px solid var(--primary);outline-offset:2px;border-radius:6px}@media(max-width:768px){#menu a.is-current,#menu a.is-current>span{border-bottom:none !important;background:var(--entry);color:var(--primary);border-radius:8px;font-weight:600}#menu a.is-current{padding:10px}#menu a{width:100%;border-radius:8px}#menu .active{border-bottom:none !important}}body.menu-open{overflow:hidden;touch-action:none}</style><script>(function(){const s=document.getElementById("menu"),t=document.getElementById("menu-toggle"),o=document.getElementById("menu-overlay");if(!s||!t||!o)return;const i=()=>window.innerWidth<=768,r=()=>{document.body.classList.add("menu-open"),t.setAttribute("aria-expanded","true"),o.hidden=!1,l()},n=()=>{document.body.classList.remove("menu-open"),t.setAttribute("aria-expanded","false"),o.hidden=!0,d(),t.focus()};t.addEventListener("click",()=>{const e=document.body.classList.contains("menu-open");e?n():r()}),o.addEventListener("click",n),window.addEventListener("keydown",e=>{e.key==="Escape"&&n()}),window.addEventListener("resize",()=>{window.innerWidth>768&&n()});const c=()=>{const e=Array.from(document.querySelectorAll(".has-submenu")),t=t=>{e.forEach(e=>{if(e!==t&&e.classList.contains("active")){e.classList.remove("active");const t=e.querySelector(".submenu-toggle");t&&t.setAttribute("aria-expanded","false")}})};e.forEach(e=>{const o=e.querySelector("a"),n=e.querySelector(".submenu-toggle"),a=e.querySelector(".submenu");if(!n||!a||!o)return;const r=t=>{n.setAttribute("aria-expanded",t?"true":"false"),e.classList.toggle("active",t)},s=()=>{const n=!e.classList.contains("active");n&&t(e),r(n)};n.addEventListener("click",e=>{if(!i())return;e.preventDefault(),e.stopPropagation(),s()}),n.addEventListener("keydown",e=>{if(!i())return;(e.key==="Enter"||e.key===" ")&&(e.preventDefault(),e.stopPropagation(),s())}),o.addEventListener("click",e=>{if(!i())return;e.preventDefault(),e.stopPropagation(),s()})})};let e=[];const a=t=>{if(t.key!=="Tab"||!document.body.classList.contains("menu-open"))return;e=Array.from(s.querySelectorAll('a, button, [tabindex="0"]'));const n=e[0],o=e[e.length-1];if(!n||!o)return;t.shiftKey&&document.activeElement===n?(t.preventDefault(),o.focus()):!t.shiftKey&&document.activeElement===o&&(t.preventDefault(),n.focus())},l=()=>{e=Array.from(s.querySelectorAll('a, button, [tabindex="0"]')),e.length&&e[0].focus(),document.addEventListener("keydown",a)},d=()=>{document.removeEventListener("keydown",a)};c(),s.addEventListener("click",e=>{const t=e.target;if(!(t instanceof Element)||!i())return;const s=t.closest("a");if(!s)return;const o=s.closest("li"),a=o&&o.classList.contains("has-submenu");a||n()})})()</script><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Rust Async Microservices, Tamed: Rate Limiting, Backpressure, Batching, and Middleware</h1><div class=post-description>When traffic surges like a flood, equip your Rust microservices with four gates—rate limiting, backpressure, batching, and middleware—using Tokio + Tower. Keep P99 steady and make speed happen in order.</div><div class=post-meta><span title='2025-08-20 10:00:00 +0800 +0800'>August 20, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1130 words&nbsp;·&nbsp;Rexai Programming&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://rexai.top/tutorials/rust/rust-async-microservices-traffic-gates-20250820/>Zh-Cn</a></li></ul></div></header><figure class=entry-cover><img loading=eager srcset='https://rexai.top/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover_hu_921334c103e92705.png 360w,https://rexai.top/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover_hu_6f57a988886873fc.png 480w,https://rexai.top/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover_hu_803b33aecad6c193.png 720w,https://rexai.top/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover.png 800w' src=https://rexai.top/tutorials/rust/rust-async-microservices-traffic-gates-20250820/cover.png sizes="(min-width: 768px) 720px, 100vw" width=800 height=533 alt="Rust async microservices: rate limiting, backpressure, batching and middleware"><figcaption>Install four traffic gates with Tower/Axum</figcaption></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#why-congestion-happens-async-is-not-infinite-concurrency aria-label="Why congestion happens: async is not infinite concurrency">Why congestion happens: async is not infinite concurrency</a></li><li><a href=#rate-limiting-the-turnstile-at-the-entrance aria-label="Rate limiting: the turnstile at the entrance">Rate limiting: the turnstile at the entrance</a></li><li><a href=#backpressure-when-the-kitchen-rail-is-full-stop-taking-orders aria-label="Backpressure: when the kitchen rail is full, stop taking orders">Backpressure: when the kitchen rail is full, stop taking orders</a></li><li><a href=#batching-fill-a-bucket-then-flushmore-throughput-less-effort aria-label="Batching: fill a bucket, then flush—more throughput, less effort">Batching: fill a bucket, then flush—more throughput, less effort</a></li><li><a href=#middleware-rules-in-one-brain-elegance-everywhere aria-label="Middleware: rules in one brain, elegance everywhere">Middleware: rules in one brain, elegance everywhere</a></li><li><a href=#choosing-a-strategy-a-oneliner-matrix aria-label="Choosing a strategy: a one‑liner matrix">Choosing a strategy: a one‑liner matrix</a></li><li><a href=#one-practical-flow-keeping-checkout-stable-on-a-big-sale aria-label="One practical flow: keeping checkout stable on a big sale">One practical flow: keeping checkout stable on a big sale</a></li><li><a href=#observe-and-optimize-the-stability-flywheel aria-label="Observe and optimize: the stability flywheel">Observe and optimize: the stability flywheel</a></li><li><a href=#final-hammer-stability-isnt-slowits-ordered-speed aria-label="Final hammer: stability isn’t slow—it’s ordered speed">Final hammer: stability isn’t slow—it’s ordered speed</a></li></ul></div></details></div><div class=post-content><p>Bold claim up front: stability is not “slow.” Stability is making “fast” happen in an orderly way. With the gates placed at the right spots, even a flood can be channeled into rivers.</p><p>Think about a subway during Monday rush hour. Entry turnstiles regulate inflow, platforms enforce headcount, trains add extra cars, and the PA system orchestrates everything. That is exactly what microservices must do under high concurrency. Your Rust service needs the same four gates: rate limiting, backpressure, batching, and a smart brain of middleware.</p><h2 id=why-congestion-happens-async-is-not-infinite-concurrency>Why congestion happens: async is not infinite concurrency<a hidden class=anchor aria-hidden=true href=#why-congestion-happens-async-is-not-infinite-concurrency>#</a></h2><p>Async doesn’t mean your server can accept infinite work like it’s cheating. Reality looks more like a food delivery platform: a burst of orders gets constrained by riders, kitchen capacity, and merchant speed. Throughput is always limited by the slowest stage. Queues are not sinful by themselves—uncontrolled queues cause incidents.</p><p>First principle: don’t let requests pile up where you don’t see them. Reject when you must, queue when you should, batch when it helps, coordinate when it matters.</p><h2 id=rate-limiting-the-turnstile-at-the-entrance>Rate limiting: the turnstile at the entrance<a hidden class=anchor aria-hidden=true href=#rate-limiting-the-turnstile-at-the-entrance>#</a></h2><p>Entrance rate limiting is like the turnstile. We’re not unfriendly—we just don’t want everyone to rush the platform and trample each other. Limiting protects yourself and downstream dependencies, especially third‑party APIs, databases, and expensive resources.</p><p>In Rust with the Tower ecosystem, you can add this gate cleanly. Here’s a minimal stack that bundles rate limiting, concurrency caps, timeouts, and tracing, reusable across Axum routes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>use</span> axum::{routing::post, Router};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> std::time::Duration;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower::{ServiceBuilder, timeout::TimeoutLayer};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower::limit::{ConcurrencyLimitLayer, RateLimitLayer};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower_http::trace::TraceLayer;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>create_order</span>() -&gt; <span style=color:#66d9ef>&amp;</span>&#39;static <span style=color:#66d9ef>str</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;ok&#34;</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#[tokio::main]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> middleware_stack <span style=color:#f92672>=</span> ServiceBuilder::new()
</span></span><span style=display:flex><span>        .layer(TraceLayer::new_for_http())
</span></span><span style=display:flex><span>        .layer(TimeoutLayer::new(Duration::from_secs(<span style=color:#ae81ff>2</span>)))
</span></span><span style=display:flex><span>        .layer(ConcurrencyLimitLayer::new(<span style=color:#ae81ff>256</span>))
</span></span><span style=display:flex><span>        .layer(RateLimitLayer::new(<span style=color:#ae81ff>100</span>, Duration::from_secs(<span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>        .into_inner();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> app <span style=color:#f92672>=</span> Router::new()
</span></span><span style=display:flex><span>        .route(<span style=color:#e6db74>&#34;/orders&#34;</span>, post(create_order))
</span></span><span style=display:flex><span>        .layer(middleware_stack);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    axum::Server::bind(<span style=color:#f92672>&amp;</span><span style=color:#e6db74>&#34;0.0.0.0:3000&#34;</span>.parse().unwrap())
</span></span><span style=display:flex><span>        .serve(app.into_make_service())
</span></span><span style=display:flex><span>        .<span style=color:#66d9ef>await</span>
</span></span><span style=display:flex><span>        .unwrap();
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Two layers are doing heavy lifting here: RPS smoothing (max 100 per second) and in‑flight capping (max 256 concurrent). This both smooths bursts and prevents downstream collapse. You can go further with tenant/API‑key buckets to make fairness measurable.</p><h2 id=backpressure-when-the-kitchen-rail-is-full-stop-taking-orders>Backpressure: when the kitchen rail is full, stop taking orders<a hidden class=anchor aria-hidden=true href=#backpressure-when-the-kitchen-rail-is-full-stop-taking-orders>#</a></h2><p>When the kitchen ticket rail is full, a good manager doesn’t keep jamming tickets in. They ask customers to retry later or go to another window. That’s backpressure: “I’m busy now—give me a breath.”</p><p>In Rust, the first hammer is boundedness. Whether it’s a queue or a buffer, set an upper bound. When full, either wait or fail fast:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>use</span> tokio::sync::mpsc;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tokio::time::{sleep, Duration};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#[derive(Debug)]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>struct</span> <span style=color:#a6e22e>Job</span>(<span style=color:#66d9ef>u64</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#[tokio::main]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Bounded queue with capacity 1024
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> (tx, <span style=color:#66d9ef>mut</span> rx) <span style=color:#f92672>=</span> mpsc::channel::<span style=color:#f92672>&lt;</span>Job<span style=color:#f92672>&gt;</span>(<span style=color:#ae81ff>1024</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Producer: try_send fails if the queue is full; we fail fast here
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> producer <span style=color:#f92672>=</span> tokio::spawn(<span style=color:#66d9ef>async</span> <span style=color:#66d9ef>move</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#66d9ef>in</span> <span style=color:#ae81ff>0</span><span style=color:#f92672>..</span><span style=color:#ae81ff>10_000</span><span style=color:#66d9ef>u64</span> {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> tx.try_send(Job(i)).is_err() {
</span></span><span style=display:flex><span>                <span style=color:#75715e>// Backpressure signal: drop or record and let upstream retry
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    });
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Consumer: simulate processing latency
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> consumer <span style=color:#f92672>=</span> tokio::spawn(<span style=color:#66d9ef>async</span> <span style=color:#66d9ef>move</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>let</span> Some(job) <span style=color:#f92672>=</span> rx.recv().<span style=color:#66d9ef>await</span> {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> _ <span style=color:#f92672>=</span> job;
</span></span><span style=display:flex><span>            sleep(Duration::from_millis(<span style=color:#ae81ff>2</span>)).<span style=color:#66d9ef>await</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    });
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> _ <span style=color:#f92672>=</span> tokio::<span style=color:#a6e22e>join!</span>(producer, consumer);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>At the HTTP entrance, add “busy? instantly 503” to tell upstream to back off instead of building a latency snowball:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>use</span> tower::ServiceBuilder;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower::load_shed::LoadShedLayer;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> app <span style=color:#f92672>=</span> Router::new()
</span></span><span style=display:flex><span>    .route(<span style=color:#e6db74>&#34;/orders&#34;</span>, post(create_order))
</span></span><span style=display:flex><span>    .layer(ServiceBuilder::new()
</span></span><span style=display:flex><span>        .layer(LoadShedLayer::new())
</span></span><span style=display:flex><span>    );
</span></span></code></pre></div><p>The biggest pitfall is unboundedness. Unbounded queues drag tail latency into the unacceptable; blind retries under load create retry storms. Make failures arrive sooner and make retries smarter (exponential backoff with caps).</p><h2 id=batching-fill-a-bucket-then-flushmore-throughput-less-effort>Batching: fill a bucket, then flush—more throughput, less effort<a hidden class=anchor aria-hidden=true href=#batching-fill-a-bucket-then-flushmore-throughput-less-effort>#</a></h2><p>A laundromat doesn’t run a machine for one shirt; couriers deliver multiple packages to the same block in one run. Many write paths benefit from batching: bulk DB writes, batch MQ publishes, or downstream APIs that support batch endpoints.</p><p>A common approach is dual thresholds: flush when you hit N items or T milliseconds—whichever comes first. Skeleton:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>use</span> tokio::{sync::mpsc, time::{self, Duration, Instant}};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#[derive(Clone, Debug)]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>struct</span> <span style=color:#a6e22e>Event</span>(String);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#[tokio::main]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> (tx, <span style=color:#66d9ef>mut</span> rx) <span style=color:#f92672>=</span> mpsc::channel::<span style=color:#f92672>&lt;</span>Event<span style=color:#f92672>&gt;</span>(<span style=color:#ae81ff>2048</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> batcher <span style=color:#f92672>=</span> tokio::spawn(<span style=color:#66d9ef>async</span> <span style=color:#66d9ef>move</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> max_batch <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span><span style=color:#66d9ef>usize</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> max_wait <span style=color:#f92672>=</span> Duration::from_millis(<span style=color:#ae81ff>50</span>);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> buf <span style=color:#f92672>=</span> Vec::with_capacity(max_batch);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> deadline <span style=color:#f92672>=</span> Instant::now() <span style=color:#f92672>+</span> max_wait;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> ticker <span style=color:#f92672>=</span> time::interval(max_wait);
</span></span><span style=display:flex><span>        ticker.set_missed_tick_behavior(time::MissedTickBehavior::Delay);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>loop</span> {
</span></span><span style=display:flex><span>            tokio::<span style=color:#a6e22e>select!</span> {
</span></span><span style=display:flex><span>                maybe <span style=color:#f92672>=</span> rx.recv() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>match</span> maybe {
</span></span><span style=display:flex><span>                        Some(ev) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>                            buf.push(ev);
</span></span><span style=display:flex><span>                            <span style=color:#66d9ef>if</span> buf.len() <span style=color:#f92672>&gt;=</span> max_batch {
</span></span><span style=display:flex><span>                                flush(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> buf).<span style=color:#66d9ef>await</span>;
</span></span><span style=display:flex><span>                                deadline <span style=color:#f92672>=</span> Instant::now() <span style=color:#f92672>+</span> max_wait;
</span></span><span style=display:flex><span>                            }
</span></span><span style=display:flex><span>                        }
</span></span><span style=display:flex><span>                        None <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>                            <span style=color:#66d9ef>if</span> <span style=color:#f92672>!</span>buf.is_empty() { flush(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> buf).<span style=color:#66d9ef>await</span>; }
</span></span><span style=display:flex><span>                            <span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>                        }
</span></span><span style=display:flex><span>                    }
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>                _ <span style=color:#f92672>=</span> ticker.tick() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> <span style=color:#f92672>!</span>buf.is_empty() { flush(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> buf).<span style=color:#66d9ef>await</span>; }
</span></span><span style=display:flex><span>                    deadline <span style=color:#f92672>=</span> Instant::now() <span style=color:#f92672>+</span> max_wait;
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    });
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> _ <span style=color:#f92672>=</span> batcher.<span style=color:#66d9ef>await</span>;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>flush</span>(buf: <span style=color:#66d9ef>&amp;</span><span style=color:#a6e22e>mut</span> Vec<span style=color:#f92672>&lt;</span>Event<span style=color:#f92672>&gt;</span>) {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Batch write to downstream (DB/queue/batch API)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// Record batch size and latency for tuning
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    buf.clear();
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Batching boosts throughput but you must watch tail latency. Start with small batches and short waits, observe P95/P99, then dial up slowly. Emit metrics for batch size, wait time, and failure rate.</p><h2 id=middleware-rules-in-one-brain-elegance-everywhere>Middleware: rules in one brain, elegance everywhere<a hidden class=anchor aria-hidden=true href=#middleware-rules-in-one-brain-elegance-everywhere>#</a></h2><p>Airport security has multiple stations, coordinated but non‑blocking. Middleware is where you centralize rules: timeouts, retries, rate limiting, compression, tracing, auth. Configure once, benefit many times.</p><p>In Tower/Axum, a common stack looks like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>use</span> std::time::Duration;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower::{ServiceBuilder, timeout::TimeoutLayer};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower::limit::{ConcurrencyLimitLayer, RateLimitLayer};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower_http::{trace::TraceLayer, compression::CompressionLayer, classify::ServerErrorsFailureClass};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tower_http::cors::CorsLayer;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> middleware_stack <span style=color:#f92672>=</span> ServiceBuilder::new()
</span></span><span style=display:flex><span>    .layer(TraceLayer::new_for_http())
</span></span><span style=display:flex><span>    .layer(CompressionLayer::new())
</span></span><span style=display:flex><span>    .layer(CorsLayer::permissive())
</span></span><span style=display:flex><span>    .layer(TimeoutLayer::new(Duration::from_secs(<span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>    .layer(ConcurrencyLimitLayer::new(<span style=color:#ae81ff>256</span>))
</span></span><span style=display:flex><span>    .layer(RateLimitLayer::new(<span style=color:#ae81ff>100</span>, Duration::from_secs(<span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>    .into_inner();
</span></span></code></pre></div><p>Pro tip: extract parameters into config and make them metric‑driven. When the system is hot, automatically shrink the concurrency cap; when downstream is healthy, ease it open. Tower makes this natural.</p><h2 id=choosing-a-strategy-a-oneliner-matrix>Choosing a strategy: a one‑liner matrix<a hidden class=anchor aria-hidden=true href=#choosing-a-strategy-a-oneliner-matrix>#</a></h2><ul><li>If upstream is too aggressive: apply rate limiting at the entrance, then backpressure to prevent buildup.</li><li>If downstream is expensive: prioritize batching with timeouts and smart retries.</li><li>If the entire chain is jittery: steady the entrance with rate limiting, then make concurrency/timeout/retry parameters dynamic and metric‑driven.</li></ul><h2 id=one-practical-flow-keeping-checkout-stable-on-a-big-sale>One practical flow: keeping checkout stable on a big sale<a hidden class=anchor aria-hidden=true href=#one-practical-flow-keeping-checkout-stable-on-a-big-sale>#</a></h2><ul><li>At the entrance, rate limit by tenant/user and reserve burst budgets for power users.</li><li>Writes use batching: “100 items or 50 ms, whichever first.”</li><li>Place a concurrency cap before inventory service; when busy, instantly 503 (load shed). Upstream retries with exponential backoff.</li><li>Use tracing for end‑to‑end spans and metrics for exposure. Dashboards focus on in/out QPS, queue depth, batch size, timeout rate, and P95/P99.</li></ul><h2 id=observe-and-optimize-the-stability-flywheel>Observe and optimize: the stability flywheel<a hidden class=anchor aria-hidden=true href=#observe-and-optimize-the-stability-flywheel>#</a></h2><p>Without data, tuning is like walking in the dark. At minimum, capture: in/out traffic, current concurrency, queue depth, batch size, timeouts/retries, and P95/P99 latency. Incidents should make it obvious which segment is red. With these, you can automate: when SLO alerts fire, temporarily reduce entrance RPS by 20% or shorten batch wait by 30% to ride out peaks, then restore.</p><h2 id=final-hammer-stability-isnt-slowits-ordered-speed>Final hammer: stability isn’t slow—it’s ordered speed<a hidden class=anchor aria-hidden=true href=#final-hammer-stability-isnt-slowits-ordered-speed>#</a></h2><p>Install the four gates on your critical endpoints. Put the gate at the entrance so floods don’t slam the kitchen. Then you’ll find that even under massive load, the system is “busy but orderly.”</p></div><section class=post-actions aria-label="post actions"><div class=post-actions__group><div class=post-actions__item><script type=text/javascript src=https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js data-name=bmc-button data-slug=winbbryz data-color=#FFDD00 data-emoji data-font=Cookie data-text="Buy me a coffee" data-outline-color=#000000 data-font-color=#000000 data-coffee-color=#ffffff></script></div></div></section><footer class=post-footer><ul class=post-tags><li><a href=https://rexai.top/en/tags/tokio/>Tokio</a></li><li><a href=https://rexai.top/en/tags/axum/>Axum</a></li><li><a href=https://rexai.top/en/tags/tower/>Tower</a></li><li><a href=https://rexai.top/en/tags/rate-limiting/>Rate-Limiting</a></li><li><a href=https://rexai.top/en/tags/backpressure/>Backpressure</a></li><li><a href=https://rexai.top/en/tags/batching/>Batching</a></li><li><a href=https://rexai.top/en/tags/middleware/>Middleware</a></li></ul></footer></article></main><footer class=footer><span>© <a href=https://rexai.top/>梦兽编程</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>